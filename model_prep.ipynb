{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch import nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.metrics import precision_score, f1_score, accuracy_score, recall_score, classification_report\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "df = pd.read_csv('with_4th_dataset.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "lb = LabelEncoder()\r\n",
    "df['label'] = lb.fit_transform(df['label'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\r\n",
    "    def __init__(self, df) -> None:\r\n",
    "        super().__init__()\r\n",
    "        self.y = torch.tensor(df.pop('label').values, dtype=torch.long)\r\n",
    "        self.X = torch.unsqueeze(torch.tensor(df.iloc[:,:].values, dtype=torch.float32), dim=1)\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        return self.X[idx], self.y[idx]\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "init_data = MyDataset(df)\r\n",
    "n = len(init_data)\r\n",
    "lengths = [int(n*0.8), n-int(n*0.8)]\r\n",
    "train_set, val_set = random_split(init_data, lengths=lengths)\r\n",
    "len(train_set), len(val_set), n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1199, 300, 1499)"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=8)\r\n",
    "val_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "x,y = next(iter(train_loader))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1, 1, 4, 4, 1, 1, 3, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# class Model(nn.Module):\r\n",
    "#     def __init__(self):\r\n",
    "#         super().__init__()\r\n",
    "#         self.cnn = nn.Sequential(\r\n",
    "#             nn.Conv1d(1,1000, 7, 2),\r\n",
    "#             nn.ReLU(),\r\n",
    "#             nn.Dropout(0.2),\r\n",
    "#             nn.Conv1d(1000, 500, 5, 2),\r\n",
    "#             nn.ReLU(),\r\n",
    "#             nn.Dropout(0.2),\r\n",
    "#             nn.Conv1d(500, 200, 3, 2),\r\n",
    "#             nn.ReLU(),\r\n",
    "#             nn.Dropout(0.2),\r\n",
    "#             nn.MaxPool1d(7, 2),\r\n",
    "#             nn.Conv1d(200, 100, 3, 2),\r\n",
    "#             nn.ReLU(),\r\n",
    "#             nn.Dropout(0.2),\r\n",
    "#             nn.Flatten(),\r\n",
    "#             nn.Linear(15400, 500),\r\n",
    "#             nn.ReLU(),\r\n",
    "#             nn.Dropout(0.1),\r\n",
    "#             nn.Linear(500, 5),\r\n",
    "#             nn.Softmax(dim=1)\r\n",
    "#             )\r\n",
    "    \r\n",
    "#     def forward(self, x):\r\n",
    "#         return self.cnn(x)\r\n",
    "import torch.nn.functional as F\r\n",
    "class MyConv1dPadSame(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    extend nn.Conv1d to support SAME padding\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups=1):\r\n",
    "        super(MyConv1dPadSame, self).__init__()\r\n",
    "        self.in_channels = in_channels\r\n",
    "        self.out_channels = out_channels\r\n",
    "        self.kernel_size = kernel_size\r\n",
    "        self.stride = stride\r\n",
    "        self.groups = groups\r\n",
    "        self.conv = torch.nn.Conv1d(\r\n",
    "            in_channels=self.in_channels, \r\n",
    "            out_channels=self.out_channels, \r\n",
    "            kernel_size=self.kernel_size, \r\n",
    "            stride=self.stride, \r\n",
    "            groups=self.groups)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        \r\n",
    "        net = x\r\n",
    "        \r\n",
    "        # compute pad shape\r\n",
    "        in_dim = net.shape[-1]\r\n",
    "        out_dim = (in_dim + self.stride - 1) // self.stride\r\n",
    "        p = max(0, (out_dim - 1) * self.stride + self.kernel_size - in_dim)\r\n",
    "        pad_left = p // 2\r\n",
    "        pad_right = p - pad_left\r\n",
    "        net = F.pad(net, (pad_left, pad_right), \"constant\", 0)\r\n",
    "        \r\n",
    "        net = self.conv(net)\r\n",
    "\r\n",
    "        return net\r\n",
    "        \r\n",
    "class MyMaxPool1dPadSame(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    extend nn.MaxPool1d to support SAME padding\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, kernel_size):\r\n",
    "        super(MyMaxPool1dPadSame, self).__init__()\r\n",
    "        self.kernel_size = kernel_size\r\n",
    "        self.stride = 1\r\n",
    "        self.max_pool = torch.nn.MaxPool1d(kernel_size=self.kernel_size)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        \r\n",
    "        net = x\r\n",
    "        \r\n",
    "        # compute pad shape\r\n",
    "        in_dim = net.shape[-1]\r\n",
    "        out_dim = (in_dim + self.stride - 1) // self.stride\r\n",
    "        p = max(0, (out_dim - 1) * self.stride + self.kernel_size - in_dim)\r\n",
    "        pad_left = p // 2\r\n",
    "        pad_right = p - pad_left\r\n",
    "        net = F.pad(net, (pad_left, pad_right), \"constant\", 0)\r\n",
    "        \r\n",
    "        net = self.max_pool(net)\r\n",
    "        \r\n",
    "        return net\r\n",
    "    \r\n",
    "class BasicBlock(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    ResNet Basic Block\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, downsample, use_bn, use_do, is_first_block=False):\r\n",
    "        super(BasicBlock, self).__init__()\r\n",
    "        \r\n",
    "        self.in_channels = in_channels\r\n",
    "        self.kernel_size = kernel_size\r\n",
    "        self.out_channels = out_channels\r\n",
    "        self.stride = stride\r\n",
    "        self.groups = groups\r\n",
    "        self.downsample = downsample\r\n",
    "        if self.downsample:\r\n",
    "            self.stride = stride\r\n",
    "        else:\r\n",
    "            self.stride = 1\r\n",
    "        self.is_first_block = is_first_block\r\n",
    "        self.use_bn = use_bn\r\n",
    "        self.use_do = use_do\r\n",
    "\r\n",
    "        # the first conv\r\n",
    "        self.bn1 = nn.BatchNorm1d(in_channels)\r\n",
    "        self.relu1 = nn.ReLU()\r\n",
    "        self.do1 = nn.Dropout(p=0.5)\r\n",
    "        self.conv1 = MyConv1dPadSame(\r\n",
    "            in_channels=in_channels, \r\n",
    "            out_channels=out_channels, \r\n",
    "            kernel_size=kernel_size, \r\n",
    "            stride=self.stride,\r\n",
    "            groups=self.groups)\r\n",
    "\r\n",
    "        # the second conv\r\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\r\n",
    "        self.relu2 = nn.ReLU()\r\n",
    "        self.do2 = nn.Dropout(p=0.5)\r\n",
    "        self.conv2 = MyConv1dPadSame(\r\n",
    "            in_channels=out_channels, \r\n",
    "            out_channels=out_channels, \r\n",
    "            kernel_size=kernel_size, \r\n",
    "            stride=1,\r\n",
    "            groups=self.groups)\r\n",
    "                \r\n",
    "        self.max_pool = MyMaxPool1dPadSame(kernel_size=self.stride)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        \r\n",
    "        identity = x\r\n",
    "        \r\n",
    "        # the first conv\r\n",
    "        out = x\r\n",
    "        if not self.is_first_block:\r\n",
    "            if self.use_bn:\r\n",
    "                out = self.bn1(out)\r\n",
    "            out = self.relu1(out)\r\n",
    "            if self.use_do:\r\n",
    "                out = self.do1(out)\r\n",
    "        out = self.conv1(out)\r\n",
    "        \r\n",
    "        # the second conv\r\n",
    "        if self.use_bn:\r\n",
    "            out = self.bn2(out)\r\n",
    "        out = self.relu2(out)\r\n",
    "        if self.use_do:\r\n",
    "            out = self.do2(out)\r\n",
    "        out = self.conv2(out)\r\n",
    "        \r\n",
    "        # if downsample, also downsample identity\r\n",
    "        if self.downsample:\r\n",
    "            identity = self.max_pool(identity)\r\n",
    "            \r\n",
    "        # if expand channel, also pad zeros to identity\r\n",
    "        if self.out_channels != self.in_channels:\r\n",
    "            identity = identity.transpose(-1,-2)\r\n",
    "            ch1 = (self.out_channels-self.in_channels)//2\r\n",
    "            ch2 = self.out_channels-self.in_channels-ch1\r\n",
    "            identity = F.pad(identity, (ch1, ch2), \"constant\", 0)\r\n",
    "            identity = identity.transpose(-1,-2)\r\n",
    "        \r\n",
    "        # shortcut\r\n",
    "        out += identity\r\n",
    "\r\n",
    "        return out\r\n",
    "    \r\n",
    "class ResNet1D(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    Input:\r\n",
    "        X: (n_samples, n_channel, n_length)\r\n",
    "        Y: (n_samples)\r\n",
    "        \r\n",
    "    Output:\r\n",
    "        out: (n_samples)\r\n",
    "        \r\n",
    "    Pararmetes:\r\n",
    "        in_channels: dim of input, the same as n_channel\r\n",
    "        base_filters: number of filters in the first several Conv layer, it will double at every 4 layers\r\n",
    "        kernel_size: width of kernel\r\n",
    "        stride: stride of kernel moving\r\n",
    "        groups: set larget to 1 as ResNeXt\r\n",
    "        n_block: number of blocks\r\n",
    "        n_classes: number of classes\r\n",
    "        \r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, in_channels, base_filters, kernel_size, stride, groups, n_block, n_classes, downsample_gap=2, increasefilter_gap=4, use_bn=True, use_do=True, verbose=False):\r\n",
    "        super(ResNet1D, self).__init__()\r\n",
    "        \r\n",
    "        self.verbose = verbose\r\n",
    "        self.n_block = n_block\r\n",
    "        self.kernel_size = kernel_size\r\n",
    "        self.stride = stride\r\n",
    "        self.groups = groups\r\n",
    "        self.use_bn = use_bn\r\n",
    "        self.use_do = use_do\r\n",
    "\r\n",
    "        self.downsample_gap = downsample_gap # 2 for base model\r\n",
    "        self.increasefilter_gap = increasefilter_gap # 4 for base model\r\n",
    "\r\n",
    "        # first block\r\n",
    "        self.first_block_conv = MyConv1dPadSame(in_channels=in_channels, out_channels=base_filters, kernel_size=self.kernel_size, stride=1)\r\n",
    "        self.first_block_bn = nn.BatchNorm1d(base_filters)\r\n",
    "        self.first_block_relu = nn.ReLU()\r\n",
    "        out_channels = base_filters\r\n",
    "                \r\n",
    "        # residual blocks\r\n",
    "        self.basicblock_list = nn.ModuleList()\r\n",
    "        for i_block in range(self.n_block):\r\n",
    "            # is_first_block\r\n",
    "            if i_block == 0:\r\n",
    "                is_first_block = True\r\n",
    "            else:\r\n",
    "                is_first_block = False\r\n",
    "            # downsample at every self.downsample_gap blocks\r\n",
    "            if i_block % self.downsample_gap == 1:\r\n",
    "                downsample = True\r\n",
    "            else:\r\n",
    "                downsample = False\r\n",
    "            # in_channels and out_channels\r\n",
    "            if is_first_block:\r\n",
    "                in_channels = base_filters\r\n",
    "                out_channels = in_channels\r\n",
    "            else:\r\n",
    "                # increase filters at every self.increasefilter_gap blocks\r\n",
    "                in_channels = int(base_filters*2**((i_block-1)//self.increasefilter_gap))\r\n",
    "                if (i_block % self.increasefilter_gap == 0) and (i_block != 0):\r\n",
    "                    out_channels = in_channels * 2\r\n",
    "                else:\r\n",
    "                    out_channels = in_channels\r\n",
    "            \r\n",
    "            tmp_block = BasicBlock(\r\n",
    "                in_channels=in_channels, \r\n",
    "                out_channels=out_channels, \r\n",
    "                kernel_size=self.kernel_size, \r\n",
    "                stride = self.stride, \r\n",
    "                groups = self.groups, \r\n",
    "                downsample=downsample, \r\n",
    "                use_bn = self.use_bn, \r\n",
    "                use_do = self.use_do, \r\n",
    "                is_first_block=is_first_block)\r\n",
    "            self.basicblock_list.append(tmp_block)\r\n",
    "\r\n",
    "        # final prediction\r\n",
    "        self.final_bn = nn.BatchNorm1d(out_channels)\r\n",
    "        self.final_relu = nn.ReLU(inplace=True)\r\n",
    "        # self.do = nn.Dropout(p=0.5)\r\n",
    "        self.dense = nn.Linear(out_channels, n_classes)\r\n",
    "        # self.softmax = nn.Softmax(dim=1)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        \r\n",
    "        out = x\r\n",
    "        \r\n",
    "        # first conv\r\n",
    "        if self.verbose:\r\n",
    "            print('input shape', out.shape)\r\n",
    "        out = self.first_block_conv(out)\r\n",
    "        if self.verbose:\r\n",
    "            print('after first conv', out.shape)\r\n",
    "        if self.use_bn:\r\n",
    "            out = self.first_block_bn(out)\r\n",
    "        out = self.first_block_relu(out)\r\n",
    "        \r\n",
    "        # residual blocks, every block has two conv\r\n",
    "        for i_block in range(self.n_block):\r\n",
    "            net = self.basicblock_list[i_block]\r\n",
    "            if self.verbose:\r\n",
    "                print('i_block: {0}, in_channels: {1}, out_channels: {2}, downsample: {3}'.format(i_block, net.in_channels, net.out_channels, net.downsample))\r\n",
    "            out = net(out)\r\n",
    "            if self.verbose:\r\n",
    "                print(out.shape)\r\n",
    "\r\n",
    "        # final prediction\r\n",
    "        if self.use_bn:\r\n",
    "            out = self.final_bn(out)\r\n",
    "        out = self.final_relu(out)\r\n",
    "        out = out.mean(-1)\r\n",
    "        if self.verbose:\r\n",
    "            print('final pooling', out.shape)\r\n",
    "        # out = self.do(out)\r\n",
    "        out = self.dense(out)\r\n",
    "        if self.verbose:\r\n",
    "            print('dense', out.shape)\r\n",
    "        # out = self.softmax(out)\r\n",
    "        if self.verbose:\r\n",
    "            print('softmax', out.shape)\r\n",
    "        \r\n",
    "        return out    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "kernel_size = 16\r\n",
    "stride = 2\r\n",
    "n_block = 8\r\n",
    "downsample_gap = 6\r\n",
    "increasefilter_gap = 12\r\n",
    "model = ResNet1D(\r\n",
    "    in_channels=1, \r\n",
    "    base_filters=128, # 64 for ResNet1D, 352 for ResNeXt1D\r\n",
    "    kernel_size=kernel_size, \r\n",
    "    stride=stride, \r\n",
    "    groups=32, \r\n",
    "    n_block=n_block, \r\n",
    "    n_classes=5, \r\n",
    "    # downsample_gap=downsample_gap, \r\n",
    "    increasefilter_gap=increasefilter_gap, \r\n",
    "    use_do=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# model = torch.load('model.pth')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "\r\n",
    "EPOCHS = 100\r\n",
    "lr = 0.0005\r\n",
    "optimizer = torch.optim.Adam(model.parameters())\r\n",
    "loss_fn = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(device)\r\n",
    "\r\n",
    "min_loss = 0.18\r\n",
    "\r\n",
    "for i in range(EPOCHS):\r\n",
    "    total_loss = 0\r\n",
    "    model.train()\r\n",
    "    model.to(device)\r\n",
    "    for j, (data, label) in enumerate(train_loader):\r\n",
    "        out = model(data.to(device))\r\n",
    "        # print(out.shape)\r\n",
    "        # print(label.shape)\r\n",
    "        # break\r\n",
    "        loss = loss_fn(out, label.to(device))\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "      # getting training quality data\r\n",
    "        current_loss = loss.item()\r\n",
    "        total_loss += current_loss\r\n",
    "    if (total_loss/len(train_loader)) < min_loss:\r\n",
    "      min_loss = total_loss/len(train_loader)\r\n",
    "      name = 'model'+'_'+str(i) + '_'+str(min_loss)+'_.pth'\r\n",
    "      torch.save(model, name)\r\n",
    "      print('min loss changed {0}'.format(min_loss))\r\n",
    "      \r\n",
    "    print(i, total_loss/len(train_loader))\r\n",
    "  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n",
      "0 0.3170558581377069\n",
      "1 0.30464287432531517\n",
      "2 0.2898465188841025\n",
      "3 0.292579785871009\n",
      "4 0.28590234187742075\n",
      "min loss changed 0.2775438614562154\n",
      "5 0.2775438614562154\n",
      "6 0.29409376621246336\n",
      "min loss changed 0.269418256940941\n",
      "7 0.269418256940941\n",
      "min loss changed 0.26828449851522845\n",
      "8 0.26828449851522845\n",
      "9 0.27761006919046244\n",
      "10 0.2759263473500808\n",
      "min loss changed 0.24497292285164196\n",
      "11 0.24497292285164196\n",
      "12 0.2501605952406923\n",
      "13 0.2576814612125357\n",
      "14 0.2574148789917429\n",
      "15 0.28221152156591417\n",
      "16 0.2767007898228864\n",
      "17 0.2451568257684509\n",
      "18 0.26172968978683153\n",
      "19 0.269069105759263\n",
      "20 0.26455424862603344\n",
      "21 0.267681183864673\n",
      "22 0.2587560453886787\n",
      "23 0.2499283703789115\n",
      "24 0.25290167836472394\n",
      "min loss changed 0.24453802927707632\n",
      "25 0.24453802927707632\n",
      "26 0.26264225063224633\n",
      "27 0.2554334934862951\n",
      "28 0.24656437678883472\n",
      "min loss changed 0.23252926586816708\n",
      "29 0.23252926586816708\n",
      "30 0.2547215252245466\n",
      "31 0.24923076532781124\n",
      "32 0.2411634614566962\n",
      "33 0.25516813129807514\n",
      "34 0.25911834878847\n",
      "35 0.26512288657948374\n",
      "36 0.2556916973739862\n",
      "37 0.25400567477568986\n",
      "38 0.2419200277949373\n",
      "39 0.25151847979674735\n",
      "40 0.25248045824468135\n",
      "41 0.2508367224161824\n",
      "42 0.2548326668639978\n",
      "43 0.23921672609945138\n",
      "44 0.2506039616341392\n",
      "min loss changed 0.21948317099362613\n",
      "45 0.21948317099362613\n",
      "46 0.2625184135014812\n",
      "47 0.24116348148634037\n",
      "48 0.23159629303961993\n",
      "49 0.23395766045277316\n",
      "50 0.25008731610452134\n",
      "51 0.24786840720723072\n",
      "52 0.2273322669789195\n",
      "53 0.2428667833780249\n",
      "54 0.256648287003239\n",
      "55 0.24780070690438152\n",
      "56 0.2561897992094358\n",
      "57 0.22849556573977073\n",
      "58 0.22924378708004953\n",
      "59 0.27495986346155404\n",
      "60 0.25986806900550924\n",
      "61 0.27294094185034434\n",
      "62 0.2641556401240329\n",
      "63 0.2626130799079935\n",
      "64 0.25602310537050166\n",
      "65 0.2578317147865892\n",
      "66 0.23418028227984905\n",
      "67 0.2516598380729556\n",
      "68 0.23115745364998777\n",
      "69 0.2546782249150177\n",
      "70 0.25139645047485826\n",
      "71 0.25223295311133065\n",
      "72 0.2216015211492777\n",
      "73 0.23882515876243512\n",
      "74 0.24569952169433237\n",
      "75 0.23351792361587287\n",
      "76 0.2572983885059754\n",
      "77 0.23576045028865336\n",
      "min loss changed 0.21645007477452358\n",
      "78 0.21645007477452358\n",
      "79 0.2350635300949216\n",
      "80 0.2526720606163144\n",
      "81 0.2256035149997721\n",
      "82 0.22601892005031307\n",
      "83 0.23837927273164192\n",
      "84 0.2221167373160521\n",
      "85 0.23697197228049238\n",
      "86 0.23151320622613034\n",
      "87 0.22300670504570008\n",
      "88 0.24022193576519688\n",
      "min loss changed 0.2126288783038035\n",
      "89 0.2126288783038035\n",
      "90 0.22762673664838076\n",
      "91 0.2535409350413829\n",
      "92 0.2550017353147268\n",
      "93 0.24247637428343297\n",
      "94 0.2316570977354422\n",
      "95 0.23963878691739712\n",
      "96 0.23239740426962574\n",
      "min loss changed 0.2093621235092481\n",
      "97 0.2093621235092481\n",
      "98 0.21610366798316438\n",
      "99 0.2236421414402624\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# set model to evaluating (testing)\r\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\r\n",
    "val_loss = 0\r\n",
    "precision, recall, f1, accuracy = [], [], [], []\r\n",
    "model.eval()\r\n",
    "with torch.no_grad():\r\n",
    "    for i,(data, label) in enumerate(val_loader):\r\n",
    "        # print(data.shape)\r\n",
    "        \r\n",
    "        outputs = model(data.to(device))\r\n",
    "        # print(outputs.shape)\r\n",
    "        # break\r\n",
    "        val_loss += loss_fn(outputs, label.to(device))\r\n",
    "        print(val_loss/(i+1))\r\n",
    "\r\n",
    "        predicted_classes = torch.max(outputs, 1)[1] # get class from network's prediction\r\n",
    "        \r\n",
    "        # calculate P/R/F1/A metrics for batch\r\n",
    "        for acc, metric in zip((precision, recall, f1), \r\n",
    "                                (precision_score, recall_score, f1_score)):\r\n",
    "            acc.append(\r\n",
    "                metric(label.cpu(), predicted_classes.cpu(), average=\"macro\")\r\n",
    "            )\r\n",
    "        accuracy.append(accuracy_score(label.cpu(), predicted_classes.cpu()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.3708, device='cuda:0')\n",
      "tensor(0.3052, device='cuda:0')\n",
      "tensor(0.2536, device='cuda:0')\n",
      "tensor(0.2164, device='cuda:0')\n",
      "tensor(0.2006, device='cuda:0')\n",
      "tensor(0.1991, device='cuda:0')\n",
      "tensor(0.2346, device='cuda:0')\n",
      "tensor(0.2106, device='cuda:0')\n",
      "tensor(0.2023, device='cuda:0')\n",
      "tensor(0.1876, device='cuda:0')\n",
      "tensor(0.1751, device='cuda:0')\n",
      "tensor(0.2213, device='cuda:0')\n",
      "tensor(0.2049, device='cuda:0')\n",
      "tensor(0.1922, device='cuda:0')\n",
      "tensor(0.2264, device='cuda:0')\n",
      "tensor(0.2209, device='cuda:0')\n",
      "tensor(0.2105, device='cuda:0')\n",
      "tensor(0.2358, device='cuda:0')\n",
      "tensor(0.2251, device='cuda:0')\n",
      "tensor(0.2173, device='cuda:0')\n",
      "tensor(0.2140, device='cuda:0')\n",
      "tensor(0.2082, device='cuda:0')\n",
      "tensor(0.2009, device='cuda:0')\n",
      "tensor(0.2077, device='cuda:0')\n",
      "tensor(0.2113, device='cuda:0')\n",
      "tensor(0.2056, device='cuda:0')\n",
      "tensor(0.2020, device='cuda:0')\n",
      "tensor(0.1955, device='cuda:0')\n",
      "tensor(0.1906, device='cuda:0')\n",
      "tensor(0.1857, device='cuda:0')\n",
      "tensor(0.1813, device='cuda:0')\n",
      "tensor(0.2017, device='cuda:0')\n",
      "tensor(0.1969, device='cuda:0')\n",
      "tensor(0.2190, device='cuda:0')\n",
      "tensor(0.2138, device='cuda:0')\n",
      "tensor(0.2127, device='cuda:0')\n",
      "tensor(0.2157, device='cuda:0')\n",
      "tensor(0.2127, device='cuda:0')\n",
      "tensor(0.2111, device='cuda:0')\n",
      "tensor(0.2676, device='cuda:0')\n",
      "tensor(0.2611, device='cuda:0')\n",
      "tensor(0.2566, device='cuda:0')\n",
      "tensor(0.2520, device='cuda:0')\n",
      "tensor(0.2469, device='cuda:0')\n",
      "tensor(0.2417, device='cuda:0')\n",
      "tensor(0.2390, device='cuda:0')\n",
      "tensor(0.2369, device='cuda:0')\n",
      "tensor(0.2350, device='cuda:0')\n",
      "tensor(0.2322, device='cuda:0')\n",
      "tensor(0.2340, device='cuda:0')\n",
      "tensor(0.2305, device='cuda:0')\n",
      "tensor(0.2282, device='cuda:0')\n",
      "tensor(0.2269, device='cuda:0')\n",
      "tensor(0.2243, device='cuda:0')\n",
      "tensor(0.2230, device='cuda:0')\n",
      "tensor(0.2206, device='cuda:0')\n",
      "tensor(0.2173, device='cuda:0')\n",
      "tensor(0.2150, device='cuda:0')\n",
      "tensor(0.2143, device='cuda:0')\n",
      "tensor(0.2111, device='cuda:0')\n",
      "tensor(0.2101, device='cuda:0')\n",
      "tensor(0.2086, device='cuda:0')\n",
      "tensor(0.2066, device='cuda:0')\n",
      "tensor(0.2063, device='cuda:0')\n",
      "tensor(0.2041, device='cuda:0')\n",
      "tensor(0.2037, device='cuda:0')\n",
      "tensor(0.2010, device='cuda:0')\n",
      "tensor(0.2039, device='cuda:0')\n",
      "tensor(0.2012, device='cuda:0')\n",
      "tensor(0.1985, device='cuda:0')\n",
      "tensor(0.1958, device='cuda:0')\n",
      "tensor(0.1958, device='cuda:0')\n",
      "tensor(0.1936, device='cuda:0')\n",
      "tensor(0.1976, device='cuda:0')\n",
      "tensor(0.1962, device='cuda:0')\n",
      "tensor(0.1956, device='cuda:0')\n",
      "tensor(0.1942, device='cuda:0')\n",
      "tensor(0.1935, device='cuda:0')\n",
      "tensor(0.1925, device='cuda:0')\n",
      "tensor(0.1921, device='cuda:0')\n",
      "tensor(0.1920, device='cuda:0')\n",
      "tensor(0.1908, device='cuda:0')\n",
      "tensor(0.1887, device='cuda:0')\n",
      "tensor(0.1875, device='cuda:0')\n",
      "tensor(0.2012, device='cuda:0')\n",
      "tensor(0.2001, device='cuda:0')\n",
      "tensor(0.1989, device='cuda:0')\n",
      "tensor(0.1974, device='cuda:0')\n",
      "tensor(0.1960, device='cuda:0')\n",
      "tensor(0.1966, device='cuda:0')\n",
      "tensor(0.1946, device='cuda:0')\n",
      "tensor(0.1929, device='cuda:0')\n",
      "tensor(0.1913, device='cuda:0')\n",
      "tensor(0.1920, device='cuda:0')\n",
      "tensor(0.1930, device='cuda:0')\n",
      "tensor(0.1919, device='cuda:0')\n",
      "tensor(0.1919, device='cuda:0')\n",
      "tensor(0.1919, device='cuda:0')\n",
      "tensor(0.1905, device='cuda:0')\n",
      "tensor(0.1912, device='cuda:0')\n",
      "tensor(0.1899, device='cuda:0')\n",
      "tensor(0.1912, device='cuda:0')\n",
      "tensor(0.1977, device='cuda:0')\n",
      "tensor(0.1973, device='cuda:0')\n",
      "tensor(0.1960, device='cuda:0')\n",
      "tensor(0.1947, device='cuda:0')\n",
      "tensor(0.1935, device='cuda:0')\n",
      "tensor(0.1918, device='cuda:0')\n",
      "tensor(0.1905, device='cuda:0')\n",
      "tensor(0.1895, device='cuda:0')\n",
      "tensor(0.1887, device='cuda:0')\n",
      "tensor(0.1911, device='cuda:0')\n",
      "tensor(0.1917, device='cuda:0')\n",
      "tensor(0.1908, device='cuda:0')\n",
      "tensor(0.1907, device='cuda:0')\n",
      "tensor(0.1892, device='cuda:0')\n",
      "tensor(0.1882, device='cuda:0')\n",
      "tensor(0.1941, device='cuda:0')\n",
      "tensor(0.1979, device='cuda:0')\n",
      "tensor(0.1985, device='cuda:0')\n",
      "tensor(0.1980, device='cuda:0')\n",
      "tensor(0.1971, device='cuda:0')\n",
      "tensor(0.2011, device='cuda:0')\n",
      "tensor(0.1999, device='cuda:0')\n",
      "tensor(0.2005, device='cuda:0')\n",
      "tensor(0.1995, device='cuda:0')\n",
      "tensor(0.1985, device='cuda:0')\n",
      "tensor(0.1981, device='cuda:0')\n",
      "tensor(0.1981, device='cuda:0')\n",
      "tensor(0.1975, device='cuda:0')\n",
      "tensor(0.1982, device='cuda:0')\n",
      "tensor(0.1971, device='cuda:0')\n",
      "tensor(0.1963, device='cuda:0')\n",
      "tensor(0.1950, device='cuda:0')\n",
      "tensor(0.1949, device='cuda:0')\n",
      "tensor(0.1936, device='cuda:0')\n",
      "tensor(0.1938, device='cuda:0')\n",
      "tensor(0.1934, device='cuda:0')\n",
      "tensor(0.1927, device='cuda:0')\n",
      "tensor(0.1914, device='cuda:0')\n",
      "tensor(0.1907, device='cuda:0')\n",
      "tensor(0.1909, device='cuda:0')\n",
      "tensor(0.1902, device='cuda:0')\n",
      "tensor(0.1905, device='cuda:0')\n",
      "tensor(0.1917, device='cuda:0')\n",
      "tensor(0.1907, device='cuda:0')\n",
      "tensor(0.1899, device='cuda:0')\n",
      "tensor(0.1891, device='cuda:0')\n",
      "tensor(0.1886, device='cuda:0')\n",
      "tensor(0.1887, device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "[sum(x*100)//len(x) for x in [precision, recall, f1, accuracy]]\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[93.0, 93.0, 92.0, 95.0]"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "torch.save(model, 'model.pth')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "[53.0, 58.0, 52.0, 61.0] [66.0, 74.0, 68.0, 75.0] "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('eagle_labs': conda)"
  },
  "interpreter": {
   "hash": "cb14f38eb9dbddad9e02f7f9fcc4764c3f951d30ef892449ff63fbb4eacc0eb1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}